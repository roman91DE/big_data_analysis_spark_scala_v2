{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, replace\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "user_mails = [\n",
    "    (1, \"alice@example.com\"),\n",
    "    (2, \"bob@example.com\"),\n",
    "    (3, \"charlie@example.com\"),\n",
    "    (4, \"dave@example.com\"),\n",
    "    (5, \"eve@example.com\"),\n",
    "    (6, \"no@password.com\")\n",
    "]\n",
    "\n",
    "user_passwords = [\n",
    "    (1, \"pass1234\"),\n",
    "    (2, \"securePass!\"),\n",
    "    (3, \"qwerty123\"),\n",
    "    (4, \"hunter2\"),\n",
    "    (5, \"letmein\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/homebrew/Cellar/apache-spark/3.5.4/libexec\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/16 16:16:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"my awesome app\")\n",
    "sc = SparkContext(master=conf.get(\"spark.master\"), conf=conf)\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/roman/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/roman/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 72, in dumps\n",
      "    cp.dump(obj)\n",
      "    ~~~~~~~^^^^^\n",
      "  File \"/Users/roman/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 540, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ~~~~~~~~~~~~^^^^^^^^^^^\n",
      "  File \"/Users/roman/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 630, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/roman/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 503, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/roman/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 484, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "  File \"/Users/roman/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 156, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "  File \"/Users/roman/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n",
      "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/serializers.py:437\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:72\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     69\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     70\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 72\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:540\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:630\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:503\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:484\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    483\u001b[0m newargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_getnewargs(func)\n\u001b[0;32m--> 484\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (types\u001b[38;5;241m.\u001b[39mFunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    486\u001b[0m         _function_setstate)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:156\u001b[0m, in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    145\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m,\n\u001b[1;32m    154\u001b[0m }\n\u001b[0;32m--> 156\u001b[0m f_globals_ref \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m f_globals \u001b[38;5;241m=\u001b[39m {k: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    158\u001b[0m              func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m}\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/cloudpickle/cloudpickle.py:236\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    235\u001b[0m names \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_names\n\u001b[0;32m--> 236\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m], \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/rdd.py:1235\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/rdd.py:1224\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/rdd.py:1078\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 1078\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/rdd.py:949\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 949\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/rdd.py:2949\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2947\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2949\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2950\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2951\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func,\n\u001b[1;32m   2952\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier)\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_val \u001b[38;5;241m=\u001b[39m python_rdd\u001b[38;5;241m.\u001b[39masJavaRDD()\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/rdd.py:2828\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2827\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 2828\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\u001b[38;5;28mbytearray\u001b[39m(pickled_command), env, includes, sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[1;32m   2830\u001b[0m                               sc\u001b[38;5;241m.\u001b[39mpythonVer, broadcast_vars, sc\u001b[38;5;241m.\u001b[39m_javaAccumulator)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/rdd.py:2814\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc, command):\n\u001b[1;32m   2812\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 2814\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   2816\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[1;32m   2817\u001b[0m         broadcast \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(pickled_command)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataScienceEnv/lib/python3.13/site-packages/pyspark/serializers.py:447\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    446\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([0, 2, 3, 4, 6], 5)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
